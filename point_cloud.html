<!doctype html>
<html class="no-js" lang="en" data-content_root="./">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="genindex.html" /><link rel="search" title="Search" href="search.html" /><link rel="next" title="API" href="api.html" /><link rel="prev" title="Adding Your Seed Algorithms" href="seed_algorithms.html" />

    <!-- Generated with Sphinx 8.2.3 and Furo 2025.07.19 -->
        <title>Point Cloud Classification Optimization - LLM-GE 1.0 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d111a655" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo.css?v=25af2a20" />
    <link rel="stylesheet" type="text/css" href="_static/styles/furo-extensions.css?v=8dab3a3b" />
    
    


<style>
  body {
    --color-code-background: #f2f2f2;
  --color-code-foreground: #1e1e1e;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #2b2b2b;
  --color-code-foreground: #f8f8f2;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="index.html"><div class="brand">LLM-GE 1.0 documentation</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="index.html">
  
  <span class="sidebar-brand-text">LLM-GE 1.0 documentation</span>
  
</a><form class="sidebar-search-container" method="get" action="search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="seed_algorithms.html">Adding Your Seed Algorithms</a></li>
<li class="toctree-l1 current has-children current-page"><a class="current reference internal" href="#">Point Cloud Classification Optimization</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Point Cloud Classification Optimization</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul class="simple">
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="api.html">API</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of API</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="generated/run_improved.html">run_improved</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/llm_crossover.html">llm_crossover</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/llm_mutation.html">llm_mutation</a></li>
<li class="toctree-l2"><a class="reference internal" href="generated/llm_utils.html">llm_utils</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="view-this-page">
  <a class="muted-link" href="_sources/point_cloud.rst.txt" title="View this page">
    <svg><use href="#svg-eye"></use></svg>
    <span class="visually-hidden">View this page</span>
  </a>
</div>
<div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="point-cloud-classification-optimization">
<h1>Point Cloud Classification Optimization<a class="headerlink" href="#point-cloud-classification-optimization" title="Link to this heading">¶</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">¶</a></h2>
<p>How to get started and setup your system for Point Cloud Classification Team</p>
<blockquote>
<div><p>In Morris, Jurado, and Zutty’s recent paper “LLM Guided Evolution – The Automation of Models Advancing Models”,
they were able to create a framework that uses LLMs with a layer of creativity to speed up the process of evolving ML models.
Our specific goal is to generalize this framework by improving 2 current state of the start point cloud classification models.
We picked Point Transformers and PointNet++ along with the Model40Net DataSet.</p>
</div></blockquote>
<section id="point-transformers">
<h3>Point Transformers<a class="headerlink" href="#point-transformers" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>Point Transformers improves point cloud processing by using self-attention mechanisms, similar to those in transformer models for sequential data.
They apply attention to capture relationships between all points in the cloud, allowing the model to focus on relevant features and dependencies, both locally and globally, leading to better handling of irregular, unordered data.
For our purposes we’re not using the EMADE Repo instead we’ll be using the Large Language Model Guide Evolution Repo.</p>
</div></blockquote>
</section>
<section id="pointnet">
<h3>PointNet++<a class="headerlink" href="#pointnet" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>PointNet++ extends PointNet by using a hierarchical approach to capture local geometric patterns in 3D point clouds.
It samples and groups points into neighborhoods, applies PointNet-style operations to each group, and then propagates the learned features to progressively capture both local and global structures.</p>
</div></blockquote>
</section>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading">¶</a></h2>
<section id="clone-github-repository">
<h3>Clone GitHub Repository<a class="headerlink" href="#clone-github-repository" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>Head over to Github.com and clone the Large Language Model Guided Evolution Generic Repository where we’re integrating the 2 point cloud classification models.</p>
<p><a class="reference external" href="https://github.com/MosesTheRedSea/LLM-Guided-Evolution-Generic">LLMGE Repository Github</a></p>
</div></blockquote>
<img alt="_images/repo_screenshot.png" src="_images/repo_screenshot.png" />
<p><strong>Clone Repository</strong></p>
<blockquote>
<div><p>Select the main branch, when you click the green button dropdown &lt;&gt; Code</p>
</div></blockquote>
<p><strong>Terminal Commands</strong></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">mkdir LLMGE_Point_Cloud</span>

<span class="go">cd LLMGE_Point_Cloud</span>

<span class="go">git clone https://github.com/MosesTheRedSea/LLM-Guided-Evolution-Generic.git</span>
</pre></div>
</div>
</section>
<section id="pace-ice-configurations">
<h3>PACE ICE Configurations<a class="headerlink" href="#pace-ice-configurations" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>Training &amp; evaluating models takes a significant amount of computing power which your computers alone may not be able to handle. Which is why this semester we will make great use of PACE ICE.</p>
</div></blockquote>
<p><strong>What is PACE ICE?</strong></p>
<blockquote>
<div><p>PACE stands for Partnership for an Advanced Computing Environment, while ICE is the Instructional Cluster Environment.</p>
<p>ICE provides students and instructors with a high-performance computing environment for courses, including those in the College of Computing.</p>
<p>It’s a free resource for instructional purposes.</p>
</div></blockquote>
<p><strong>Important Links</strong></p>
<blockquote>
<div><p>The Georgia Tech VPN, specifically the GlobalProtect VPN, is a secure network connection that allows users to access Georgia Tech resources and services from off-campus.</p>
<p><a class="reference external" href="https://vpn.gatech.edu/global-protect/login.esp">GATECH VPN</a></p>
<p><strong>username</strong> : gatech username - without &#64;gatech.edu</p>
<p><strong>password</strong> : gatech account password</p>
<img alt="_images/vpn.png" src="_images/vpn.png" />
<ul class="simple">
<li><p>You can access any website through the VPN through the <code class="docutils literal notranslate"><span class="pre">enter</span> <span class="pre">url</span></code> dropdown.</p></li>
</ul>
<img alt="_images/enter_url.png" src="_images/enter_url.png" />
<ul class="simple">
<li><p>Enter in the pace-ice link: <a class="reference external" href="https://ondemand-ice.pace.gatech.edu/">https://ondemand-ice.pace.gatech.edu/</a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You must be on the vpn to access pace.</p>
</div>
<ul class="simple">
<li><p>You screen will be redirected to the pace-ice landing page.</p></li>
</ul>
<img alt="_images/pace-ice-login.png" src="_images/pace-ice-login.png" />
<ul class="simple">
<li><p>Click on the <code class="docutils literal notranslate"><span class="pre">Flies</span></code> dropdown in the top-left corner, then select the <code class="docutils literal notranslate"><span class="pre">Home</span> <span class="pre">Directory</span></code></p></li>
</ul>
<img alt="_images/pace-ice-directory.png" src="_images/pace-ice-directory.png" />
<ul class="simple">
<li><p>Traverse all the way to your scratch directory so that we can drag and drop our LLM-GE Repository that you cloned to your local computer.</p></li>
<li><p>Click the Upload Button at the top of the window, and drag and drop your LLMGE-Generic Repository into your scratch directory.</p></li>
<li><p>Now that we finally have the repository on PACE ICE we can start the setup.</p></li>
<li><p>From here click <code class="docutils literal notranslate"><span class="pre">Interactive</span> <span class="pre">Apps</span></code> dropdown, and select <code class="docutils literal notranslate"><span class="pre">VS</span> <span class="pre">Code</span></code> the last option on the bottom.</p></li>
</ul>
<img alt="_images/pace-ice-interactive-apps-vscode.png" src="_images/pace-ice-interactive-apps-vscode.png" />
<ul class="simple">
<li><p>Check the settings (change number of hours to <code class="docutils literal notranslate"><span class="pre">8</span></code>) then click launch.</p></li>
</ul>
<img alt="_images/pace-ice-vscode-job-configurations.png" src="_images/pace-ice-vscode-job-configurations.png" />
<ul class="simple">
<li><p>Head back to the home page, and click on <code class="docutils literal notranslate"><span class="pre">My</span> <span class="pre">Interactive</span> <span class="pre">Session</span></code>.</p></li>
</ul>
<img alt="_images/pace-ice-vscode-interactive-sessions.png" src="_images/pace-ice-vscode-interactive-sessions.png" />
<ul class="simple">
<li><p>WHen you click on the Interactive Sessions button at the top of the page, it will redirect to</p></li>
</ul>
<p>another page that displays every current running session.</p>
<ul class="simple">
<li><p>The Session you just ceated has been added to the queue, one the appropiate resoruces are avaliable PACE ICE</p></li>
</ul>
<p>will let you connect and load in the session in VSCode’s Web Applicaton.</p>
<ul class="simple">
<li><p>Once you’re in VSCode, simply click <code class="docutils literal notranslate"><span class="pre">File</span> <span class="pre">-&gt;</span> <span class="pre">Open</span> <span class="pre">Folder</span></code>, and go to the directory you saved the LLMGE Repo in your scratch or whichever folder.</p></li>
</ul>
<img alt="_images/pace-ice-scratch-folder-directory.png" src="_images/pace-ice-scratch-folder-directory.png" />
<ul class="simple">
<li><p>Before we begin integrating different architectures into LLMGE, I want to explain the codebase itself how it works, what we’re trying to optimize, and our expected results.</p></li>
</ul>
</div></blockquote>
</section>
<section id="large-language-model-guided-evolution">
<h3>Large Language Model Guided Evolution<a class="headerlink" href="#large-language-model-guided-evolution" title="Link to this heading">¶</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>Large Language Model Guided Evolution is an innovative architecture that leverages the reasoning capabilities of Large Language Models to guide evolutionary search and refinement of neural network architectures.</p></li>
<li><p>The system combines natural language reasoning, feedback-based learning, and genetic optimization techniques to automatically discover and improve neural architectures.</p></li>
</ul>
</div></blockquote>
<section id="codebase-breakdown">
<h4>Codebase Breakdown<a class="headerlink" href="#codebase-breakdown" title="Link to this heading">¶</a></h4>
<blockquote>
<div><ul class="simple">
<li><p>The codebase is organized into several key directories and files, each serving specific functions in the evolutionary process:</p></li>
</ul>
<img alt="_images/llm-ge-code-base-structure.png" src="_images/llm-ge-code-base-structure.png" />
<p><strong>sota</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>sota (State of the Art) directory holds various key information we need for LLMGE to run smoothly.</p></li>
<li><p>Model Architectures: Houses the baseline neural network architectures targeted for evolutionary improvement</p></li>
<li><p>Dataset Configurations: Contains dataset specifications and preprocessing scripts</p></li>
<li><p>Training Scripts: Includes standardized training procedures and evaluation protocols</p></li>
</ul>
</div></blockquote>
<p><strong>src</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>The main source directory contains the core configuration files:</p></li>
<li><p>Large Language Model Configuration, Specific Directories, and Dataset paths</p></li>
</ul>
</div></blockquote>
<p><strong>constants.py</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Centralized configuration file containing system parameters, hyperparameters, and global constants.</p></li>
</ul>
<img alt="_images/constants-py-info-1.png" src="_images/constants-py-info-1.png" />
<ul class="simple">
<li><p>Seed network, data files, and train file path directories listed above for global access throughout codebase.</p></li>
</ul>
<img alt="_images/constants-py-info-2.png" src="_images/constants-py-info-2.png" />
<ul class="simple">
<li><p>Large Language Model Configuration, along with Evolutionary Alogirthm predefined configurations, population size, number of generations, and weights.</p></li>
</ul>
</div></blockquote>
<p><strong>templates</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>The templates sirectory houses validation and code modification templates prompt and code generation templates used by the LLM.</p></li>
<li><p>There are a wide variety of different prompts we submit to the LLM, we have the variant templates which request for modifications to the submitted SEED file.</p></li>
<li><p>There is also a validation prompt that helps with cleaning code received from LLM, invalid syntax, unfinshed responses, invalid packages.</p></li>
</ul>
</div></blockquote>
<p><strong>llm_crossover.py</strong></p>
<blockquote>
<div><img alt="_images/llmge-llm-crossover-augment-network.png" src="_images/llmge-llm-crossover-augment-network.png" />
<ul class="simple">
<li><p>Implements LLM-guided crossover operations that intelligently combine features from parent architectures.</p></li>
</ul>
</div></blockquote>
<p><strong>llm_mutation.py</strong></p>
<blockquote>
<div><img alt="_images/llmge-llm-mutations-augment_network.png" src="_images/llmge-llm-mutations-augment_network.png" />
<ul class="simple">
<li><p>Handles LLM-directed mutation operations for architecture modification and enhancement.</p></li>
</ul>
</div></blockquote>
<p><strong>llm_utils.py</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Utility functions supporting LLM interactions, prompt processing, and response parsing.</p></li>
</ul>
<img alt="_images/llmge-llm-utils-clean-code-from-llm-code.png" src="_images/llmge-llm-utils-clean-code-from-llm-code.png" />
<ul class="simple">
<li><p>llm_utils possess all the necessary methods needed for llm code generation and quality control.</p></li>
<li><p>The clean_code_from_llm has a validation sequence that ensures the quality of code recieved from the LLM, is</p></li>
</ul>
<img alt="_images/llmge-llm-utils-generate-code.png" src="_images/llmge-llm-utils-generate-code.png" />
<ul class="simple">
<li><p>The <strong>generate_augmented_code</strong> allows for a wide variety of LLM configurations, either using the hugging-face API, or a local LLM.</p></li>
<li><p>You can specifiy eother by configuring the <strong>LLM_MODAL</strong> &amp; <strong>hugging_face</strong> values in constants.py</p></li>
</ul>
</div></blockquote>
<p><strong>run.sh</strong></p>
<blockquote>
<div><img alt="_images/llmge-run-sh-main.png" src="_images/llmge-run-sh-main.png" />
<ul class="simple">
<li><p>Primary shell script for launching evolution experiments</p></li>
</ul>
</div></blockquote>
<p><strong>run_improved.py</strong></p>
<blockquote>
<div><img alt="_images/llmge-run-improved-code-snippet.png" src="_images/llmge-run-improved-code-snippet.png" />
<ul class="simple">
<li><p>Enhanced execution script for running evolution experiments with improved features</p></li>
</ul>
</div></blockquote>
<p><strong>server.py</strong></p>
<blockquote>
<div><blockquote>
<div><ul class="simple">
<li><p>Server component for distributed Local or API LLM execution</p></li>
</ul>
</div></blockquote>
<img alt="_images/llmge-local-llm-integration-server-py.png" src="_images/llmge-local-llm-integration-server-py.png" />
<ul class="simple">
<li><p>The <strong>server.py</strong> file helps with loading the local LLM stored on PACE ICE, for us to submit the mutation prompts to.</p></li>
</ul>
<img alt="_images/llmge-local-llm-integration-server-py-2.png" src="_images/llmge-local-llm-integration-server-py-2.png" />
<ul class="simple">
<li><p>We load the Local LLM, so that we can submit prompts of different token lengths and get a reponse in a reasonable amount of time.</p></li>
</ul>
</div></blockquote>
<p><strong>mixt.sh</strong></p>
<blockquote>
<div><img alt="_images/llmge-mixt-sh-main.png" src="_images/llmge-mixt-sh-main.png" />
<ul class="simple">
<li><p>Specialized script for mixed or multi-objective evolution scenarios</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</section>
<section id="code-workflow">
<h4>Code Workflow<a class="headerlink" href="#code-workflow" title="Link to this heading">¶</a></h4>
<p><strong>ExquisiteNetV2</strong></p>
<blockquote>
<div><ul>
<li><p>This section will explain the codebase workflow with the default implemented model,  ExquisiteNetV2.</p>
<blockquote>
<div><img alt="_images/llmge-llm-mutations-augment_network.png" src="_images/llmge-llm-mutations-augment_network.png" />
<ul class="simple">
<li><p>Handles LLM-directed mutation operations for architecture modification and enhancement.</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p><strong>llm_utils.py</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Utility functions supporting LLM interactions, prompt processing, and response parsing.</p></li>
</ul>
<img alt="_images/llmge-llm-utils-clean-code-from-llm-code.png" src="_images/llmge-llm-utils-clean-code-from-llm-code.png" />
<ul class="simple">
<li><p>llm_utils possess all the necessary methods needed for llm code generation and quality control.</p></li>
<li><p>The clean_code_from_llm has a validation sequence that ensures the quality of code recieved from the LLM, is</p></li>
</ul>
<img alt="_images/llmge-llm-utils-generate-code.png" src="_images/llmge-llm-utils-generate-code.png" />
<ul class="simple">
<li><p>The <strong>generate_augmented_code</strong> allows for a wide variety of LLM configurations, either using the hugging-face API, or a local LLM.</p></li>
<li><p>You can specifiy eother by configuring the <strong>LLM_MODAL</strong> &amp; <strong>hugging_face</strong> values in constants.py</p></li>
</ul>
</div></blockquote>
<p><strong>run.sh</strong></p>
<blockquote>
<div><img alt="_images/llmge-run-sh-main.png" src="_images/llmge-run-sh-main.png" />
<ul class="simple">
<li><p>Primary shell script for launching evolution experiments</p></li>
</ul>
</div></blockquote>
<p><strong>run_improved.py</strong></p>
<blockquote>
<div><img alt="_images/llmge-run-improved-code-snippet.png" src="_images/llmge-run-improved-code-snippet.png" />
<ul class="simple">
<li><p>Enhanced execution script for running evolution experiments with improved features</p></li>
</ul>
</div></blockquote>
<p><strong>server.py</strong></p>
<blockquote>
<div><blockquote>
<div><ul class="simple">
<li><p>Server component for distributed Local or API LLM execution</p></li>
</ul>
</div></blockquote>
<img alt="_images/llmge-local-llm-integration-server-py.png" src="_images/llmge-local-llm-integration-server-py.png" />
<ul class="simple">
<li><p>The <strong>server.py</strong> file helps with loading the local LLM stored on PACE ICE, for us to submit the mutation prompts to.</p></li>
</ul>
<img alt="_images/llmge-local-llm-integration-server-py-2.png" src="_images/llmge-local-llm-integration-server-py-2.png" />
<ul class="simple">
<li><p>We load the Local LLM, so that we can submit prompts of different token lengths and get a reponse in a reasonable amount of time.</p></li>
</ul>
</div></blockquote>
<p><strong>mixt.sh</strong></p>
<blockquote>
<div><img alt="_images/llmge-mixt-sh-main.png" src="_images/llmge-mixt-sh-main.png" />
<ul class="simple">
<li><p>Specialized script for mixed or multi-objective evolution scenarios</p></li>
</ul>
</div></blockquote>
<p><em>Download Dataset</em></p>
<blockquote>
<div><ul class="simple">
<li><p>ExquisiteNetV2, a Lightweight CNN designed for image classification, tested on 15 datasets (CIFAR-10, MNIST) with 518,230 parameters, achieving 99.71% accuracy on MNIST.</p></li>
<li><p>Go into the sota (State of the art) directory and click on the ExquisiteNetV2 directory.</p></li>
<li><p>Inside this directory you will see a README.md file which contains key information for the dataset.</p></li>
</ul>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span><span class="gh"># Train Cifar-10</span>
The best weight has been in the directory <span class="sb">`weight/exp`</span>.

If you want to reproduce the result, you can follow the procedure below.
<span class="k">-</span><span class="w"> </span>__Download the cifar-10 from https://www.cs.toronto.edu/~kriz/cifar.html
<span class="k">1.</span> Download python version and unzip it.
<span class="k">2.</span> Put <span class="sb">`split.py`</span> into the directory <span class="sb">`cifar-10-python`</span>

    <span class="sb">`python split.py`</span>

    Now you get the cifar10 raw image in the directory <span class="sb">`cifar10`</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Click the link to go to the cifar10 website</p></li>
</ul>
<img alt="_images/exquisite-net-v2-dataset-py.png" src="_images/exquisite-net-v2-dataset-py.png" />
<ul class="simple">
<li><p>Make sure to sleect the <strong>CIRFAR-10 python version</strong> Link</p></li>
</ul>
<img alt="_images/exquisite-net-v2-dataset-py-2.png" src="_images/exquisite-net-v2-dataset-py-2.png" />
<ul class="simple">
<li><p>It will download the cifar-10-python.tar file to your computer</p></li>
</ul>
<img alt="_images/exquisite-net-v2-dataset-py-3.png" src="_images/exquisite-net-v2-dataset-py-3.png" />
<ul class="simple">
<li><p>Once you click on it you will see another ciar-10-bathces-py folder</p></li>
<li><p>Make sure you extact the contents of cifar-10-python.tar to your computer</p></li>
<li><p>Then head on over to the PACE ICE Instructional Cluster, then drag &amp; drop the file into the ExquisteNetV2 directory</p></li>
</ul>
<img alt="_images/exquisite-net-v2-dataset-py-4.png" src="_images/exquisite-net-v2-dataset-py-4.png" />
<ul class="simple">
<li><p>Once the file is there you will run</p></li>
</ul>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span><span class="sb">`python split.py`</span>
</pre></div>
</div>
<ul class="simple">
<li><p>This command will format the cifar-10-bathces-py folder and create another directory called cifar-10, with the test and train data.</p></li>
</ul>
</div></blockquote>
<p><em>Constants.py</em></p>
<blockquote>
<div><img alt="_images/exquisite-net-v2-constants-py-1.png" src="_images/exquisite-net-v2-constants-py-1.png" />
<ul class="simple">
<li><p>root directory is the path to LLMGE folder <strong>/home/hice1/madewolu9/scratch/llm-guided-evolution/</strong></p></li>
<li><p>data path is the path to the sota model dataset <strong>ExquisiteNetV2/cifar10</strong></p></li>
<li><p>sota path is the (state of the art) model path <strong>ExquisiteNetV2/</strong></p></li>
<li><p>seed network is the main model file for the SOTA model <strong>ExquiteNetV2/network.py</strong></p></li>
<li><p>model variable is the name of the SEED network file without .py <strong>network</strong></p></li>
<li><p>train file path is the path to the train.py file used within the SOTA model.</p></li>
</ul>
<img alt="_images/exquisite-net-v2-constants-py-2.png" src="_images/exquisite-net-v2-constants-py-2.png" />
<ul class="simple">
<li><p>llm_model specifies which Large Language Model you intend to use either one of the localy provided ones on PACE ICE, or Google’s Gemini via an API key.</p></li>
<li><p>Further down you can see the Evolution Cosntants/Params</p></li>
<li><p>This is where you can configure the number of elites, total num_generations, starting generation and population size.</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<p><strong>Evolutionary Loop</strong></p>
<blockquote>
<div><img alt="_images/run-improved-individual-deap.png" src="_images/run-improved-individual-deap.png" />
<ul class="simple">
<li><p>We initialize the toolbox which holds the key methods we plan to use during the evolutionary loop.</p></li>
</ul>
<img alt="_images/run-improved-individual-creation.png" src="_images/run-improved-individual-creation.png" />
<ul class="simple">
<li><p>This is essentially the main method within run_imrpoved that starts the model evolution.</p></li>
<li><p>We create our population and configure key parameters to our specification.</p></li>
</ul>
</div></blockquote>
<p><strong>Variant Model Files</strong></p>
<blockquote>
<div><p><strong>Local Large Language Model Setup</strong></p>
<ul class="simple">
<li><p>For LLMGE to create the variant files and valid code we have to setup the Large Language Model.</p></li>
<li><p>There are 3 files where changes need to be made, constants, server, &amp; llm_utils.py.</p></li>
</ul>
<p><em>constants.py</em></p>
<blockquote>
<div><ul class="simple">
<li><p>Within the constants.py file make sure to set the LLM_MODEL global variable to the LLM of your choice.</p></li>
<li><p>You can use  <strong>mixtral</strong>, <strong>llama</strong>, and even <strong>gemini</strong> (provide your gemini API key)</p></li>
</ul>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span><span class="sb">`os.environ[&quot;GEMINI_API_KEY&quot;]`</span> = &quot;&quot;
</pre></div>
</div>
</div></blockquote>
<p><em>server.py</em></p>
<ul class="simple">
<li><p>Inside of server.py make sure to set the LLM Model Path to the <strong>MODEL_PATH</strong> variable.</p></li>
<li><p>All Locally downloaded LLM’s are located on PACE ICE, at <strong>/storage/ice-shared/vip-vvk/llm_storage/choose-your-llm</strong> (You may need to cd .. a few times)</p></li>
</ul>
<img alt="_images/llmge-server-configure.png" src="_images/llmge-server-configure.png" />
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span><span class="sb">`MODEL_PATH = /storage/ice-shared/vip-vvk/llm_storage/mixtral`</span>
</pre></div>
</div>
<p><em>llm_utils.py</em></p>
<ul class="simple">
<li><p>llm_utils essentially deals with generating the code from the LLM, we submit a request and get a generated reponse in return.</p></li>
<li><p>However, before any request is sent we have to check the <strong>LLM_MODEL</strong> along with the <strong>code_generator</strong></p></li>
</ul>
<img alt="_images/llmge-llm-utils-configure.png" src="_images/llmge-llm-utils-configure.png" />
<ul class="simple">
<li><p>The code generator varies with whichever LLM_MODEL you plan to use.</p></li>
<li><p>If you pick mixtral, make sure to set <strong>code_generator</strong> to <strong>submit_mixtral_local</strong>, do the same for each respective <strong>LLM_MODEL</strong></p></li>
</ul>
<img alt="_images/create-individual-run-improved.png" src="_images/create-individual-run-improved.png" />
<ul class="simple">
<li><p>The <strong>create_individual method</strong> takes the seed file which has been generically integrated and creates a varaint file with a unique geneId -&gt; <strong>model_xXPAsb8bdabdyuv28f.py</strong> in a sub-directory called llmge_models, which will be submitted for evaluation.</p></li>
</ul>
</div></blockquote>
<p><strong>Seed individuals</strong></p>
<blockquote>
<div><img alt="_images/llmge-env-network-py.png" src="_images/llmge-env-network-py.png" />
<ul class="simple">
<li><p>Seed individuals are created at the very start <strong>(generation_0)</strong>, using the LLM to make modifications to the main model file <strong>network.py</strong> inside of EquisteNetV2.</p></li>
</ul>
</div></blockquote>
<p><strong>Fitness Evaluation</strong></p>
<blockquote>
<div><ul class="simple">
<li><p><strong>submit_run</strong> takes in the gene_id associated with a unique model variant file.</p></li>
<li><p>We take the <strong>TRAIN_FILE</strong> that we defined inside of the <strong>constants.py</strong> file and use it to train the seed_files in the <strong>llmge_models</strong> directory.</p></li>
<li><p>We include the appropiate parameters, for each respective sota model, and finally create the python runline to evaluate the model.</p></li>
</ul>
<img alt="_images/llmge-evaluation-network.png" src="_images/llmge-evaluation-network.png" />
<ul class="simple">
<li><p>Then we wait for jobs that were submitted, and assign the fitness to the variant model files.</p></li>
<li><p>We take the model_variant files and evaluate them using the train.py file for the model architecture.</p></li>
<li><p>The results are stored in a textfile within a directory called <strong>/results</strong>.</p></li>
</ul>
<div class="highlight-markdown notranslate"><div class="highlight"><pre><span></span><span class="sb">`f&quot;{test_acc},{total_params},{val_acc},{tr_time}&quot;`</span>
<span class="sb">`fitness = [float(r.strip()) for r in results]`</span>

<span class="gh"># TODO: get all features later (test_accuracy, total_params)</span>

<span class="sb">`fitness = [fitness[0], fitness[1]] `</span>
<span class="sb">`fitness = tuple(fitness)`</span>
</pre></div>
</div>
</div></blockquote>
<p><strong>Selection</strong></p>
<blockquote>
<div><img alt="_images/llmge-elites-code.png" src="_images/llmge-elites-code.png" />
<ul class="simple">
<li><p>Next We choose the best individuals <strong>(elites)</strong> and select for reproduction.</p></li>
<li><p>Choose the elities to use in the next generation, default <strong>#elites</strong> is 10.</p></li>
</ul>
</div></blockquote>
<p><strong>Crossover</strong></p>
<blockquote>
<div><img alt="_images/llmge-crossover-code.png" src="_images/llmge-crossover-code.png" />
<ul class="simple">
<li><p>We perform a crossover, we combine code/parameters from two models to create offspring.</p></li>
<li><p>Pairs of individuals are selected.</p></li>
<li><p>For each pair, a crossover operation is performed:</p></li>
<li><p>Combine elements <strong>(code segments, parameters)</strong> from both parents, often with LLM assistance/templates.</p></li>
<li><p>Create a new gene ID, generate new code <strong>(as a Python file)</strong>, and submit a new job.</p></li>
<li><p>The new individual <strong>(“offspring”)</strong> is tracked.</p></li>
</ul>
</div></blockquote>
<p><strong>Mutation</strong></p>
<blockquote>
<div><img alt="_images/llmge-custom-mutation.png" src="_images/llmge-custom-mutation.png" />
<ul class="simple">
<li><p>Randomly alter code/parameters in a model to create a mutant</p></li>
<li><p>Each individual has a chance <strong>(with some probability)</strong> to be mutated:</p></li>
<li><p>Change hyperparameters <strong>(input_filename, output_filename, template_txt, top_p, temperature, …) (again, often LLM-guided)</strong>.</p></li>
<li><p>Create a new gene ID, generate new code, submit a new training job.</p></li>
<li><p>Track the mutated individual.</p></li>
</ul>
</div></blockquote>
<p><strong>Next Generation</strong></p>
<blockquote>
<div><img alt="_images/llmge-new-generation.png" src="_images/llmge-new-generation.png" />
<ul class="simple">
<li><p>Build new population, remove duplicates, keep elites</p></li>
<li><p>The next generation’s population is composed of:</p></li>
<li><p>Elites (best models from the previous generation)</p></li>
<li><p>Offspring from crossover/mutation</p></li>
<li><p>Duplicates are removed to ensure diversity</p></li>
</ul>
</div></blockquote>
</section>
<section id="point-transformers-integration">
<h4>Point Transformers Integration<a class="headerlink" href="#point-transformers-integration" title="Link to this heading">¶</a></h4>
<p><strong>Download Dataset</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>ModelNet40Resampled Download Link</p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/datasets/chenxaoyu/modelnet-normal-resampled">https://www.kaggle.com/datasets/chenxaoyu/modelnet-normal-resampled</a></p></li>
<li><p>Save dataset to this path <cite>data/modelnet40_normal_resampled/</cite></p></li>
</ul>
</div></blockquote>
<p><strong>constants.py</strong></p>
<blockquote>
<div><img alt="_images/point-transformers-llmge-constants-py.png" src="_images/point-transformers-llmge-constants-py.png" />
<ul class="simple">
<li><p>Here are the important files, directories, &amp; paths to integrate Point Transformers into LLMGE.</p></li>
<li><p>root directory is the path to LLMGE folder <strong>/home/hice1/madewolu9/scratch/llm-guided-evolution/</strong></p></li>
<li><p>data path is the path to the sota model dataset <strong>Point-Transformers/data/modelnet40_normal_resampled</strong></p></li>
<li><p>sota path is the (state of the art) model path <strong>Point-Transformers/</strong></p></li>
<li><p>seed network is the main model file for the SOTA model <strong>Point-Transformers/model.py</strong></p></li>
<li><p>model variable is the name of the SEED network file without .py <strong>model</strong></p></li>
<li><p>train file path is the path to the train.py file used within the SOTA model.</p></li>
</ul>
</div></blockquote>
<p><strong>run.sh</strong></p>
<blockquote>
<div><img alt="_images/point-transformers-llmge-run-sh.png" src="_images/point-transformers-llmge-run-sh.png" />
<ul class="simple">
<li><p>Shell script needed to load in the right CUDA, conda, activate the correct environment, initialize a server to start the local LLM, and run the framework</p></li>
</ul>
</div></blockquote>
<p><strong>mixt.sh</strong></p>
<blockquote>
<div><img alt="_images/point-transformers-llmge-mixt-sh.png" src="_images/point-transformers-llmge-mixt-sh.png" />
<ul class="simple">
<li><p>Shell script needed to run the LLM-augmented python files on the model</p></li>
</ul>
</div></blockquote>
<p><strong>run_improved.py</strong></p>
<blockquote>
<div><img alt="_images/point-transformers-llmge-run-improved-submit-run-py.png" src="_images/point-transformers-llmge-run-improved-submit-run-py.png" />
<ul class="simple">
<li><p>This file is key for the LLMGE framework</p></li>
<li><p>For the Point-Transformers implementation we need to replace the runline with the one that takes in the right arguments, <strong>variant_model_file</strong></p></li>
</ul>
</div></blockquote>
<p><strong>model.py</strong></p>
<blockquote>
<div><img alt="_images/point-transformers-llmge-model-py.png" src="_images/point-transformers-llmge-model-py.png" />
<ul class="simple">
<li><p>Modified train file to run all augmented Point-Transformers model files</p></li>
<li><p>Added implementation to run a certain seed on the models</p></li>
</ul>
</div></blockquote>
<p><strong>train.py</strong></p>
<blockquote>
<div><img alt="_images/point-transformers-llmge-train-cls-py-1.png" src="_images/point-transformers-llmge-train-cls-py-1.png" />
<ul class="simple">
<li><p>Model Loading grabbing correct variant model file</p></li>
</ul>
<img alt="_images/point-transformers-llmge-train-cls-py-2.png" src="_images/point-transformers-llmge-train-cls-py-2.png" />
<ul class="simple">
<li><p>Outputting model results once evaluation is concluded</p></li>
</ul>
<blockquote>
<div><ul class="simple">
<li><p>Added section to print results to a text file that measures the best epoch’s train accuracy, test accuracy, and class accuracy, and time each training cycle takes for that seed</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</section>
<section id="pointnet-integration">
<h4>PointNet++ Integration<a class="headerlink" href="#pointnet-integration" title="Link to this heading">¶</a></h4>
<p><strong>Download Dataset</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>To download the dataset go to <a class="reference external" href="https://www.kaggle.com/datasets/chenxaoyu/modelnet-normal-resampled">https://www.kaggle.com/datasets/chenxaoyu/modelnet-normal-resampled</a></p></li>
<li><p>Then save the dataset in <cite>data/modelnet40_normal_resampled/</cite></p></li>
</ul>
</div></blockquote>
<p><strong>Constants.py</strong></p>
<blockquote>
<div><img alt="_images/pointnet-constants.png" src="_images/pointnet-constants.png" />
<ul class="simple">
<li><dl class="simple">
<dt>This shows us what is needed from the model for it to run:</dt><dd><ul>
<li><p>ROOT_DIR: sets the location of where our framework is located</p></li>
<li><p>DATA_PATH: sets the location of where our data is located, for people using PACE-ICE the location is already preset so they can save storage</p></li>
<li><p>SOTA_ROOT: sets the location of where our state of the art (SOTA) machine is and accesses the right one</p></li>
<li><p>SEED_NETWORK: The architecture of the model that we are augmenting</p></li>
<li><p>MODEL: Name of the SEED_NETWORK so that we can limit the number of changes we need to make in run_improved.py if we want to switch between models</p></li>
<li><p>TRAIN_FILE: The training file to train the augmented model on the data</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><strong>run.sh</strong></p>
<blockquote>
<div><img alt="_images/pointnet-bash.png" src="_images/pointnet-bash.png" />
<ul class="simple">
<li><p>Shell script needed to load in the right CUDA, conda, activate the correct environment, initialize a server to start the local LLM, and run the framework</p></li>
</ul>
</div></blockquote>
<p><strong>mixt.sh</strong></p>
<blockquote>
<div><img alt="_images/pointnet-mixt.png" src="_images/pointnet-mixt.png" />
<ul class="simple">
<li><p>Shell script needed to run the LLM-augmented python files on the model</p></li>
</ul>
</div></blockquote>
<p><strong>run_improved.py</strong></p>
<blockquote>
<div><img alt="_images/pointnet-runline.png" src="_images/pointnet-runline.png" />
<ul class="simple">
<li><p>This file is the backbone of the framework</p></li>
<li><p>For the pointnet++ implementation we need to replace the runline with the one that takes in the right arguments and is for pointnet++</p></li>
</ul>
</div></blockquote>
<p><strong>train_classification.py</strong></p>
<blockquote>
<div><img alt="_images/pointnet-train-1.png" src="_images/pointnet-train-1.png" />
<img alt="_images/pointnet-train-2.png" src="_images/pointnet-train-2.png" />
<img alt="_images/pointnet-train-3.png" src="_images/pointnet-train-3.png" />
<ul class="simple">
<li><p>Modified train file to run all augmented pointnet model files</p></li>
<li><dl class="simple">
<dt>Modified sections:</dt><dd><ul>
<li><p>Added implementation to run a certain seed on the models</p></li>
<li><p>Redirected paths to access the dataset and the correct augmented files to run specific seed</p></li>
<li><p>Added timer to calculate how long each run training cycle takes for a specific seed</p></li>
<li><p>Added section to print results to a text file that measures the best epoch’s train accuracy, test accuracy, and class accuracy, and time each training cycle takes for that seed</p></li>
<li><p>Added implementation to use best available core no matter the device the model is run on</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p><strong>pointnet_cls_ssg.py</strong></p>
<blockquote>
<div><ul class="simple">
<li><p>Restructed the format for how this file works, the original implementation had all the necessary methods and classes needed for augmentation in a separate file which was useful for the author of this model to just call the file and import the needed methods and classes</p></li>
<li><p>To restructure, I copied all necessary methods and classes from pointnet_utils.py that were called in pointnet_cls_ssg.py so the LLM will know which methods and classes need to be augmented</p></li>
<li><dl class="simple">
<dt>Restructured Code:</dt><dd><ul>
<li><p>square_distance(src, dst): finds the pairwise squared Euclidean distance between two sets of points</p></li>
<li><p>index_points(points, idx): indexes a set of points using their provided indicies</p></li>
<li><p>farthest_point_sample(xyz, npoint): a method of sampling to select well spaced points</p></li>
<li><p>query_ball_point(radius, nsample, xyz, new_xyz): finds a ball of certain radius centered at sampled points</p></li>
<li><p>sample_and_group(npoint, radius, nsample, xyz, points, returnfps=False): uses the above methods to give each sampled point a local grouping</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
<div class="toctree-wrapper compound">
</div>
</section>
</section>
</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="api.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">API</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="seed_algorithms.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Adding Your Seed Algorithms</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2025, Clint Morris, Jason Zutty, et al.
            </div>
            Made with <a href="https://www.sphinx-doc.org/">Sphinx</a> and <a class="muted-link" href="https://pradyunsg.me">@pradyunsg</a>'s
            
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Point Cloud Classification Optimization</a><ul>
<li><a class="reference internal" href="#overview">Overview</a><ul>
<li><a class="reference internal" href="#point-transformers">Point Transformers</a></li>
<li><a class="reference internal" href="#pointnet">PointNet++</a></li>
</ul>
</li>
<li><a class="reference internal" href="#setup">Setup</a><ul>
<li><a class="reference internal" href="#clone-github-repository">Clone GitHub Repository</a></li>
<li><a class="reference internal" href="#pace-ice-configurations">PACE ICE Configurations</a></li>
<li><a class="reference internal" href="#large-language-model-guided-evolution">Large Language Model Guided Evolution</a><ul>
<li><a class="reference internal" href="#codebase-breakdown">Codebase Breakdown</a></li>
<li><a class="reference internal" href="#code-workflow">Code Workflow</a></li>
<li><a class="reference internal" href="#point-transformers-integration">Point Transformers Integration</a></li>
<li><a class="reference internal" href="#pointnet-integration">PointNet++ Integration</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="_static/documentation_options.js?v=f2a433a1"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/furo.js?v=46bd48cc"></script>
    </body>
</html>